This project focuses on stroke classification using deep learning and video-based analysis, leveraging a comprehensive set of computer vision, machine learning, and transformer-based libraries to train and evaluate an end-to-end model. Video data is processed using tools such as MoviePy, OpenCV, PyAV, and torchvision, enabling efficient video loading, frame extraction, resizing, normalization, and handling of temporal information. The dataset is managed using the Hugging Face Datasets library with support for disk-based storage and Parquet formats, allowing scalable and efficient data preprocessing and loading. The core model architecture is built using Vision Encoder–Decoder frameworks and VideoMAE models from the Transformers library, enabling spatiotemporal feature extraction from video frames and effective classification of stroke-related patterns. Training is conducted using PyTorch, with utilities such as custom loss functions, padding sequences, data loaders, and GPU acceleration to optimize performance. Label encoding and evaluation metrics are handled using scikit-learn and Evaluate, ensuring accurate multi-class stroke classification and robust performance tracking. Advanced training strategies such as early stopping, custom callbacks, and fine-tuned training arguments are employed through Hugging Face’s Trainer and Seq2SeqTrainer APIs to prevent overfitting and improve generalization. Overall, this project demonstrates a scalable and robust approach to video-based stroke classification, combining state-of-the-art transformer models, efficient video preprocessing, and structured training pipelines to accurately identify and classify stroke patterns from visual data.
